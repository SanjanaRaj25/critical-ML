<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>

    <link rel="stylesheet" href="styles.css">

     <!--Import Google Icon Font-->
     <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
     <!--Import materialize.css-->
     <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">

     <!--Let browser know website is optimized for mobile-->
     <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
</head>

<body>

    <nav>
        <div class="nav-wrapper">
          <ul id="nav-mobile" class="hide-on-med-and-down">
            <li><a href="#start"><i class="material-icons white-text">home</i></a></li>
            <li><a href="#abstract">1. Intro</a></li>
            <li><a href="#lit">2. Literature</a></li>
            <li><a href="#mistral">3. Mistral</a></li>
            <li><a href="#exp">4. Experimentation</a></li>
            <li><a href="#takeaways">5. Takeaways</a></li>
            
          </ul>
        </div>
      </nav>
            

    
 
    <section class="hidden container" id="start">
        <br><br>
        <div class="z-depth-5 container" style="width: 500; height: 500px;" id="glass">
                
            <h1 class="title center">CRITICAL <br>MACHINE <br>LEARNING</h1>
        </div>
        <a class="btn-floating btn-large waves-effect waves-light transparent right" href="#abstract"><i class="material-icons white-text">arrow_forward</i></a>
    </section>


    
    <section class="hidden container" id="abstract">
        <br><br>
        <span><h1>Introduction</h1> </span>
        <p className="grey-text">what is this project, why am I interested in it, what am I trying to accomplish</p>
        <!-- <p>I want to understand exactly what emerging large language
            models are learning “from” and what type of future these shape for marginalized communities.</p> -->
            <a class="btn-floating btn-large waves-effect waves-light transparent right" href="#lit"><i class="material-icons white-text">arrow_forward</i></a>
    </section>

    <section class="hidden container"id="lit">
        <br><br>
        <h1>key literature</h1>
        <p>click on a citation to view annotations</p>

        <ul class="collapsible">
            <li>
                <div class="collapsible-header grey lighten-3 black-text"><p id="cite">Bender, Emily M., et al. “On the dangers of stochastic parrots.” Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, Mar. 2021, https://doi.org/10.1145/3442188.3445922. </p></div>
                <div class="collapsible-body"><p class="blockquote" id="glass">“an LM is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning: a stochastic parrot” (615).
                </p><span id="annotation">The authors define a language model as referring to systems which are trained on string prediction tasks. While the move towards word embeddings in supervised NLP tasks reduced the amount of labeled training data necessary,
                     transformer models “continuously benefit from larger architectures and larger quantities of data” (611). They argue that language models continuously scale in regards to the size of training data and what spheres they impact– both of these types of growth carry particular types of risk. First, is the environmental
                      and financial cost. They explain that while the average human is responsible for about 5t in CO2 emissions per year, training a transformer model (in 2020) emitted  284t of CO2. They note that the negative externalities of climate change impact the most marginalized communities first– those that most language 
                      technology is simply not built to serve. <br> <br>The paper goes on to problematize commonly used training data: how uncurated, Internet-based datasets encode a worldview that further alienates people at the margins. The paper refutes the idea that “largeness” in the training dataset correlates to diversity or fairness. 
                      Internet access in unequally distributed, and commonly scraped sources of data such as Reddit, Twitter, and Wikipedia have incredibly unequal user bases. Filtering training data can often amplify exclusion. In the case of GPT-3, training data was filtered out if it was too dissimilar to GPT-2’s training data. 
                      The act of training on past data can cause “value-lock,” where the model learns from and thus reifies older and less inclusive ways of being and knowing. The authors emphasize the necessity of being intentional and curatorial with what is included in training data, rather than training at the largest scale possible
                       and attempting to weed out “bad” data. The article goes on to explain that as humans, our communication relies on some interpretation of implicit meaning, or a shared common ground. In the case of machine language, the comprehension of this implicit meaning is simply an “illusion” (616). There are subtle forms of bias, encoded into training data, that cannot be filtered out. This risks the generation of more biased text, “stochastically parroted” with the illusion of coherence and meaning.</span></div>
            </li>

            <li>
                <div class="collapsible-header grey lighten-4 black-text"><p id="cite">Birhane, Abeba, Vinay Uday Prabhu, and Emmanuel Kahembwe. "Multimodal datasets: misogyny, pornography, and malignant stereotypes." arXiv preprint arXiv:2110.01963 (2021).</p></div>
                <div class="collapsible-body">
                    <p class="blockquote" id="glass">“The culture in machine learning is such that ideas that promise improvements in training speed, model size or top-k accuracy improvements are rapidly embraced while ideas and revelations pertaining to unethical aspects of datasets are either ignored or take a long time to lead to changes (10)”</p>
                    <span id="annotation">This text grounds itself in the current moment of the “multi-modality drive” within AI. It describes the “vision-text dyad” as tuples containing an image, a textual description of the image, and the image’s meta-data. The data-set of these dyads is described as “internet sized,” produced by crawling the web. Broad problems in these datasets include bias in curation, problematic content within images, and labels that are imbued with bias. The “Common Crawl” is an organization that regularly crawls the World Wide Web and generates datasets of “snapshot data-dumps” that are frequently used in training pipelines for projects such as GPT-3. The most recent CommonCrawl contained “over 200,000 documents from unreliable news sites and banned subreddit pages containing hate speech and racism” (Birhane et al 3). In this study, Birhane, Prabhu, and Kahembwe analyze the LAION-400M dataset (a multimodal filtered version of the Common-Crawl dataset). When querying the dataset with non-NSFW queries, they found a high ratio of results containing explicit content and sexual violence (even after CLIP filtering). The creation of the dataset is split into 3 steps: the drive for creation, a large base source, and a filtering mechanism. With LAION-400M, images are filtered out based on the cosine similarity between the text-description and the image embeddings. The authors note that this allows the downstream propagation of offensive mis-associations. They offer examples where the cosine similarity of offensive and inaccurate labels and an image were higher than with the accurate description. There is an asymmetry between the cheapness of mining and aggregating large datasets and the high labor cost to filter it and detoxify it downstream. The authors note the large discrepancy between how fast people adopt new approaches in training speed or model size, while ignoring revelations about ethics within datasets or ML. They note the emotional toll of sifting through a dataset filled with NSFW and violent images. In the era of deep learning, large-scale AI models can be viewed as “compressed representations” of their training data. Even once a model is fine tuned and specialized, this data is still disseminated to users downstream. The text critiques the assumption that Artificial General Intelligence should be obtained by training models with “all available data” when that data contains images and text that are racially violent, illegal, and misrepresentative. What do we gain by feeding models images of sexual violence? Automatic filtering mechanisms always have a non-zero error rate, and at the scale of billions of data points, this has significant ethical implications.</span>
                </div>
            </li>

            <li>
                <div class="collapsible-header grey lighten-3 black-text"><p id="cite">Buolamwini, Joy. Unmasking AI My Mission to Protect What Is Human in a World of Machines. Random House, 2023. </p></div>
                <div class="collapsible-body">
                    <p class="blockquote" id="glass">quote</p>
                    <span id="annotation">annotation</span>
                </div>
            </li>
            
            <li>
                <div class="collapsible-header grey lighten-4 black-text"><p id="cite">Chun, Wendy Hui Kyong. Programmed Visions: Software and Memory. Cambridge: MIT Press, 2011.</p></div>
                <div class="collapsible-body"><p class="blockquote" id="glass">"New media proliferates 'programmed visions,'' which seek to shape
                    and to predict — indeed to embody — a future based on past data" (Chun 5).</p><span id="annotation">In this text, Wendy Chun explains that new media both survives and thrives on “cycles of obsolescence and renewal” (Chun 1). Programming has “hardened” into software thorough memory. Computers can be seen as vehicles of power: with the potential of creating alternative visions and interrogating the past. Software itself represents something both procedural and unknowable. Its “vapory materialization” embodies a method to “navigate our increasingly complex world” (Chun 2). They offer an interface for us to map ourselves into the totality of global capitalism, for our own subjectivities and patterns to be mapped by hidden algorithms. This shift, as Chun argues, is a response to the changing relations between subjects and objects, as computing becomes a “neoliberal governmental technology” (Chun 6). Computers are central to the management of populations and security apparatuses– for example, they drive the statistical analysis of population “health.” Using past data to shape and predict the future, computers turn the future into something that can be “bought and sold,” a world of “Laplacian determinism” (Chun 9). The programmed vision of the title is the vision of the all knowing algorithm, based on past data. This vision is always inadequate, always incomplete. Software creates a form of memory that can only be accessed when it is re-rendered: killed then re-animated. It is always in “media res.” It can only be understood in terms of what is graspable, in front of us.</span></div>
            </li>
            <li>
                <div class="collapsible-header grey lighten-3 black-text"><p id="cite">Dobson, James E. The Birth of Computer Vision. University of Minnesota Press, 2023.</p></div>
                <div class="collapsible-body"><p class="blockquote" id="glass">"In the twenty-­ first century, we have witnessed the emergence of new forms of computerized surveillance systems, high-­ tech policing, and automated decision-­making systems that have increasingly become entangled, functioning together as a new technological apparatus of social control (1)"</p><span id="annotation">This book offers a historicization of modern computer vision systems, by tracing many of the algorithms that make up modern-day computer vision applications through a genealogical lens. Dobson examines how these models were shaped by the militarized political and social contexts of the time and place of their creation. In the introduction, Dobson argues that institutions present high-tech tools as “distant, objective viewers of data” (Dobson 2). These systems are highly visual: they pull from streams of surveillance video, social media, and personal computers. They are always seeing, and changing how we see. They can “make the invisible visible,” detect objects that are hidden or obscured from the human gaze (Dobson 3). This imaginary gaze requires the obfuscation of location, history, and situatedness. The cultural and intellectual histories of algorithms are entangled with their impacts. The “models of the world” that algorithms impose back onto our world bring with them “the biases and assumptions of other historical moments” (Dobson 20). That is, machine learning recursively visits the past and carries it into the present, into the future. In Chapter 1, Dobson discusses the formation of Computer Vision as a field. This field requires vision and seeing to be redefined as the “neutral extraction of information from data” (Dobson 29). It produces new ontological theories to decipher and analyze- a way of seeing that creates and “concretizes” digital representations of objects. In Chapter 2, Dobson discusses the creation of the “Perceptron,” an early neural network algorithm invented by Frank Rosenblatt, a psychologist at Cornell University. Rosenblatt’s dissertation was a behavioral study, where he surveyed adult men and women on the behaviors of their caregivers, with a basic assumption that their future success was directly linked to their home environments as children. He created a method called the “k-coefficient” that took in the paper forms containing survey responses, and stored squared differences. In 1957, he built on this work in a published report that introduced the concept of the Perceptron, the first machine learning algorithm for artificial neurons. In Chapter 3, Dobson explores image recognition tasks. He begins by describing the work of Martin A. Fischler, an early, military funded, computer vision researcher. Fischler noted the distinction between “seeing” as a passive reception of data and “perceiving” as the active process of decomposing a visual scene into units of meaning. Later, Fischler and his Lockheed colleague Oscar Firschein worked on grammar-based methods to describe visual objects and scenes. The goal of this work was to locate “military targets of interest” (Dobson 109). Fischler extended this work of grammar based pictorial structures to facial recognition. Common contemporary methods for identifying faces from images comes from this work. This work makes normative assumptions about what a “face” or particular scene looks like– it relies on symbolic abstractions and exclusions. In Chapter 4, Dobson discusses military automatons and line detection, through a focus on the Shakey Project. Shakey was a mobile, general purpose robot that led to the discovery and popularization of the Hough transform. In the Coda, Dobson writes that the algorithms of analysis in this book were chosen because they continue to be used in the present, that the “network of histories residing in these technologies are activated through everyday encounters with computer vision” (Dobson 176). From this text, I formed key insights on how to consider the “cast of characters” relevant to the creation of an algorithm, the operators, knowledge workers, and inventors, and the value of understanding the historical roots of any technical system we rely on.</span></div>
            </li>
            
            <li>
                <div class="collapsible-header grey lighten-4 black-text"><p id="cite">Dobson, J.E. On reading and interpreting black box deep neural networks. Int J Digit Humanities 5, 431–449 (2023). https://doi.org/10.1007/s42803-023-00075-w.</p></div>
                <div class="collapsible-body"><p class="blockquote" id="glass">"Like data and code, complex deep learning models, to some degree, can be interrogated
                    and read" (437).</p><span id="annotation">This essay offers various strategies for researchers to interpret Transformers (large neural networks with multiple layers). Transformers are often recognized as black boxes, referring to their “complicated architectures and opaque decision-making mechanisms” (Dobson 430). This allows corporations to disavow agency in the creation and training of models, and limit critics' ability to access and interrogate what’s going on internally. Dobson proposes that we understand Transformers as interpretive instruments, hermeneutic in nature. Transformers are deep learning models that utilize a particular architecture: input data is converted into a word embedding, which is fed into an encoder which maps the input sequences to a series of representations, and a decoder that generates an output sequence and probabilities. The “learning” in these models takes place in “multiple locations and especially within the network component called attention heads” (Dobson 435). The outputs from multiple locations/heads are combined and transferred to higher layers. Due to the large number of dimensions in transformer embeddings, it isn’t possible to visualize relationships between embeddings without reducing or obscuring a great deal of information. While most models share a similar suite of basic evaluation tools such as confusion matrices and accuracy scores, the question of how a model comes to a particular answer, what it has learned and valued, goes unanswered. Treating the model/algorithm as an entity that is “withholding” information allows a rethinking of critique. Dobson notes that even in the case of models that are released as open-source projects, datasets are undefined: because they are private, or have been illicitly scraped. Accessing this data can reveal details about the model that simply studying its architecture cannot. Prompt engineering can lead to the leakage of training data, and reveal what norms have been encoded into a model. Training data can signal towards the ontology of a model, how its tasks ought to be framed. Adversarial testing with disruptive boundary objects can expose ambiguities in the model’s results. Hidden layers and attention heads can be targeted for critical reading, with the “higher layers” recognizing more complex language features than the “lower layers.” Visualization heat maps can show “the degree to which a particular head has learned a relation between specific tokens” (Dobson 444). It is deeply important to not discard the task of interpreting black box/deep learning models as futile, as though they are unexplainable, true models of reality. They are complex, but not beyond probing.</span></div>
            </li>
            <li>
              <div class="collapsible-header grey lighten-3 black-text"><p id="cite">Elam, Michele. “Poetry will not optimize; or, what is literature to AI?” American Literature, vol. 95, no. 2, 17 Mar. 2023, pp. 281–303, https://doi.org/10.1215/00029831-10575077. </p></div>
              <div class="collapsible-body"><div id="glass">AI, like all technologies, is a crucible of our world views, our social priorities, commitments, investments, and aspirations. As such, perhaps one of its greatest usesis to allow it to reflect us back to ourselves (295)</div><span id="annotation">Elam argues that AI-generated and augmented literature cannot be dismissed. Although writing generated by LLMs can sometimes seem eerily similar to what a human might produce, there are key distinctions. The “database” cannot tolerate data that is indeterminate, unspeakable, though that is precisely what narrative seeks to reveal. The computer embodies the “view from nowhere”— it is implied to be omniscient and ahistorical, while the human narrative acknowledges the subjective view, grounded by context and place. The first section of the text recounts the “literary consequences of algorithmic ahistoricity.” Literature does not represent history as a linear series of data points, but as a “palimpsest,” or an “ongoing  and dynamic negotiation between pasts and presents” (Elam 286). Conversely, the data used in training sets is reduced to something the computer can interpret. For instance, when GPT-3 was asked to continue lines of a Maya Angelou poem, the output was a mishmash of Black vernacular across periods of time and expressions that were incoherent together. Language is significant because it carries a certain frame of existence. It “indexes experience, and form takes the shape of its need” (Elam 288). AI generated language reduces the reader’s ability to see and understand their own realities and experiences. It can perhaps perform literature, but cannot capture its meaning. AI databases present “neutral descriptions of the world rather than its own world.” (Elam 292). The world that AI portrays is one of many: with an embedded ontology and epistemology that cannot simply stand in for the world as a whole.
            </span></div>
            
            </li>
            <li>
                <div class="collapsible-header grey lighten-4 black-text"><p id="cite">Elam, Michele. “Signs taken for wonders: AI, art & the matter of Race.” Daedalus, vol. 151, no. 2, 2022, pp. 198–217, https://doi.org/10.1162/daed_a_01910. </p></div>
                <div class="collapsible-body"><p class="blockquote" id="glass">PUT QUOTE HERE</p><span id="annotation">Lorem ipsum dolor sit amet.</span></div>
            </li>
            <li>
                <div class="collapsible-header grey lighten-3 black-text"><p id="cite">Manzini, Thomas, et al. “Black is to criminal as Caucasian is to police: Detecting and removing multiclass bias in word embeddings.” Proceedings of the 2019 Conference of the North, 2019, https://doi.org/10.18653/v1/n19-1062. </p></div>
                <div class="collapsible-body"><p class="blockquote" id="glass">PUT QUOTE HERE</p><span id="annotation">Lorem ipsum dolor sit amet.</span></div>
            </li>
           
            <li>
                <div class="collapsible-header grey lighten-4 black-text"><p id="cite">Noble, Safiya Umoja. Algorithms of Oppression How Search Engines Reinforce Racism. New York University Press, 2018.</p></div>
                <div class="collapsible-body"><p class="blockquote" id="glass">"While we often think of terms such as 'big data' and 'algorithms' as being benign, neutral, or objective, they are anything but" (Noble 1).</p>
                    <span id="annotation">This book uses the framing of “algorithmic oppression” and “technological redlining” to understand how the human decisions and biases that underscore algorithmically driven software create and deepen inequalities. In the introduction, Noble references a moment during her marketing career when her google search for the term “black girls” yielded fetishizing, pornographic sites, as the top results. Noble writes that “there is a missing social and human context in some types of algorithmically driven decision making, and this matters for everyone engaging with these types of technologies in everyday life” (Noble 10). Throughout the text, Noble offers several screenshots of search results– where a benign search about a particular identity group led to an outcome that was offensive and derived from stereotypes. In the first chapter, Noble explores how corporations control public information. Companies such as Google have the ability to “prioritize web search results” based on their business interests. The top ranking search results are not necessarily the most popular– and even when they are, they aren’t necessarily the most accurate. In the second chapter, Noble argues that White American identity is seen as a default, a norm against which all other identifications are seen as strange and aberrant. Technological racialization is framed as a particular form of algorithmic oppression, where White supremacy is the dominant lens for sense-making of race on the web. Pre-existing institutional relations situate women and people of color “outside the power systems” that technology emerges from (Noble 108). Notions of colorblindness and meritocracy obscure the structural exclusion of Black women from having space in technology, and the impacts of that exclusion. In Chapter 3, Noble discusses the search history of Dylann Roof, a 21 year old White nationalist, who killed multiple people at a primarily Black Episcopal Church. In his manifesto, Roof cited being “awakened” when search results of “black on White crime” lead to the Council of Conservative Citizens– a white supremacist group that decries “race mixing” and the “decline of white, European civilization” (Noble 112). Rather than being led to FBI statistics, Roof was brought to a propaganda site that convinced him of an epidemic of Black on White violence. This site wasn’t the most popular, or most relevant to his query. In Chapter 5, Noble examines how identities are classified in web-indexing. These systems of attempting to understand people and societies hold the “power biases of those who are able to propagate such systems” (Noble 137). Terms such as “savages” or “superior race” or “illegal” are imbued with unequal power relations. Search doesn’t just present reality, it structures and reshapes knowledge in accordance to political and cultural values. I’d like to build upon Noble’s goal of understanding the “implications of the artificial intelligentsia for people who are already systematically marginalized and oppressed’ (Noble 3). What social and human contexts are missing in algorithmically driven decision-making, and what impact does their absence have? How do algorithms of oppression impact what data is fed into Large Language Models and how they behave?</span></div>
            </li>
   
            <li>
                <div class="collapsible-header grey lighten-4 black-text"><p id="cite">Offert, Fabian, and Peter Bell. “Perceptual bias and technical metapictures: Critical Machine Vision as a Humanities Challenge.” AI &amp; SOCIETY, vol. 36, no. 4, 12 Oct. 2020, pp. 1133–1144, https://doi.org/10.1007/s00146-020-01058-z. </p></div>
                <div class="collapsible-body"><p class="blockquote" id="glass">PUT QUOTE HERE</p><span id="annotation">Lorem ipsum dolor sit amet.</span></div>
            </li>


          </ul>


            <!-- <blockquote>
                “Literature tends to resist representing history as the static, self-explanatory, sequential data points that are grist for predictive algorithms.” - Michele Elam
            </blockquote> -->

        <!-- <blockquote>
            “The oldest poems in the Western tradition, the Iliad and the Odyssey, begin with an invocation to the muse, a plea for a mysterious, unfathomable other to enter the artist, taking over, conjuring language. GPT-3 is a mysterious, unfathomable other, taking over, conjuring language” - Stephen Marche
        </blockquote> -->

  
       <a class="btn-floating btn-large waves-effect waves-light transparent right" href="#mistral"><i class="material-icons white-text">arrow_forward</i></a>
    </section>

    <section class="hidden container" id="mistral">
        <br><br>
        <h1>case study: mistral</h1>
       <p className="grey-text">1. what is mistral/history</p>
       <p className="grey-text">2. what models exist</p>
       <p className="grey-text">3. types of experimentation</p>
       <p className="grey-text">4. comparisons of prompts/responses</p>
       <p className="grey-text">5. interactive attention visualizations with next word prediction</p>
       <p className="grey-text">6. what are the downstream implications of this? what types of knowing have been compressed into the model and how could this impact real people in diff use cases?</p>
       <a class="btn-floating btn-large waves-effect waves-light transparent right" href="#exp"><i class="material-icons white-text">arrow_forward</i></a>
    </section>

    <section class="hidden container" id="exp">
        <br><br>
        <h1>more experimentation</h1>
       <p className="grey-text">maybe? the mental health experiment with different characters/interactive chat mode</p>
       <a class="btn-floating btn-large waves-effect waves-light transparent right" href="#takeaways"><i class="material-icons white-text">arrow_forward</i></a>
    </section>

    <section class="hidden container" id="takeaways">
        <br><br>
        <h1>takeaways</h1>
        <p className="grey-text">what did I learn/what can be learned from this</p>
      
       <a class="btn-floating btn-large waves-effect waves-light transparent right" href="#start"><i class="material-icons white-text">arrow_forward</i></a>
    </section>

    <section class="hidden container" id="quotes">
        <br><br>
        <h1>quotes</h1>
        <p>"literature does
            not aspire to a seamless user experience. In fact, it turns our attention
            to those <u>seams we are seduced into not seeing</u>" (Elam 284)</p>

        <p>"it is
            as if we are being forced to live in the imagination of a very few" - Ruha Benjamin</p>
       
        <p>"AI, like all technologies,
            is a crucible of our world views, our social priorities, commitments,
            investments, and aspirations. As such, perhaps one of its greatest uses
            is to allow it to reflect us back to ourselves." -Michele Elam</p>

        <p>"In accepting large amounts of web text as ‘representative’ of ‘all’ of humanity we risk perpetuating dominant viewpoints, increasing power imbalances, and further reifying inequality” - Emily Bender et al</p>
        <p>Theorist Adrian Mackenzie
            aptly calls software a “ neighbourhood of relations ” ; “ in code and coding, ”
            he argues, “relations are assembled, dismantled, bundled and dispersed within and
            across contexts.”</p>
    
</body>

    <script defer src="app.js"></script>
     <!-- Compiled and minified JavaScript -->
     <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>


</html>